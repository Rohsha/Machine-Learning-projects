{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso \n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"CE802_Ass_2019_Data.csv\")\n",
    "test = pd.read_csv(\"CE802_Ass_2019_Test.csv\")\n",
    "train1= pd.read_csv(\"CE802_Ass_2019_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1         0\n",
       "F2         0\n",
       "F3         0\n",
       "F4         0\n",
       "F5         0\n",
       "F6         0\n",
       "F7         0\n",
       "F8         0\n",
       "F9         0\n",
       "F10        0\n",
       "F11        0\n",
       "F12        0\n",
       "F13        0\n",
       "F14        0\n",
       "F15        0\n",
       "F16        0\n",
       "F17        0\n",
       "F18        0\n",
       "F19        0\n",
       "F20      172\n",
       "Class      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1         0\n",
       "F2         0\n",
       "F3         0\n",
       "F4         0\n",
       "F5         0\n",
       "F6         0\n",
       "F7         0\n",
       "F8         0\n",
       "F9         0\n",
       "F10        0\n",
       "F11        0\n",
       "F12        0\n",
       "F13        0\n",
       "F14        0\n",
       "F15        0\n",
       "F16        0\n",
       "F17        0\n",
       "F18        0\n",
       "F19        0\n",
       "F20      178\n",
       "Class    500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
       "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "final_train_nonstd=train1\n",
    "train[['F1', 'F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20']]=StandardScaler().fit_transform(train[['F1', 'F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20']])\n",
    "test[['F1', 'F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20']]=StandardScaler().fit_transform(test[['F1', 'F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20']])\n",
    "\n",
    "final_train_std=train\n",
    "final_test_std=test\n",
    "final_test_std.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1       True\n",
       "2      False\n",
       "3      False\n",
       "4       True\n",
       "       ...  \n",
       "495     True\n",
       "496    False\n",
       "497    False\n",
       "498     True\n",
       "499     True\n",
       "Name: Class, Length: 500, dtype: bool"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_nonstd1 = final_train_nonstd.drop('F20',axis=1)\n",
    "final_train_nonstd1 = final_train_nonstd1.drop('Class',axis=1)\n",
    "final_train_std1 = final_train_std.drop('F20',axis=1)\n",
    "final_train_std1 = final_train_std1.drop('Class',axis=1)\n",
    "\n",
    "final_train_drop_F20_target=train[\"Class\"]\n",
    "\n",
    "final_train_nonstd1\n",
    "final_train_drop_F20_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyKNN(final_train_drop_F20_std,final_target_dropF20):\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import model_selection, metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"coming in again\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    #dividing by 80-20 ratio\n",
    "    X_train, X_test, y_train, y_test = train_test_split(final_train_drop_F20_std, final_target_dropF20, test_size=0.2,random_state=0) \n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    clf1 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "    clf1.fit(final_train_drop_F20_std, final_target_dropF20)\n",
    "    Y_predict1=clf1.predict(final_train_drop_F20_std)\n",
    "    print(Y_predict1)\n",
    "    print(\"KNN analysis\")\n",
    "    print(\"Data is divided in to training and testing in 80:20 ratio\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy of data tested with 80:20 ratio:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    #using K-fold\n",
    "    print(\"Data is divided using cross validation for training and testing\")\n",
    "    #model report\n",
    "    k_sco = []\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    scores = cross_val_score(knn, final_train_drop_F20_std, final_target_dropF20, cv=10, scoring='accuracy')\n",
    "    print(np.mean(scores))\n",
    "    print('classification report below')\n",
    "    print(classification_report(final_target_dropF20, Y_predict1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0\n",
      " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "clf1 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "clf1.fit(final_train_std1,final_target_dropF20)\n",
    "Y_predict1=clf1.predict(final_train_std1)\n",
    "\n",
    "print(Y_predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def applySVM(final_train_drop_F20_std,final_target_dropF20):\n",
    " #   from sklearn.model_selection import RepeatedKFold\n",
    "  #  from sklearn.model_selection import KFold, cross_val_score\n",
    "   # from sklearn import svm\n",
    "   # from sklearn.metrics import classification_report\n",
    "   # from sklearn.model_selection import train_test_split\n",
    "   # X_train, X_test, y_train, y_test = train_test_split(final_train_drop_F20_std, final_target_dropF20, test_size=0.2,random_state=0) \n",
    "   # clf = svm.SVC(gamma=0.0001,C=100) \n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_pred = clf.predict(X_test)\n",
    "    #clf1 = svm.SVC(gamma=0.0001,C=100)\n",
    "    #clf1.fit(final_train_drop_F20_std, final_target_dropF20)\n",
    "    #Y_predict1=clf1.predict(final_train_drop_F20_std)\n",
    "\n",
    "    #from sklearn import metrics\n",
    "    #print(\"SVM Analysis\")\n",
    "    #print(\"Data is divided in to training and testing in 80:20 ratio\")\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    #from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "    #print(\"Accuracy of data tested with 80:20 ratio:\",accuracy_score(y_test, y_pred))\n",
    "    #print(\"Data is divided using cross validation for training and testing\")\n",
    "    #kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "    #k_fold = KFold(n_splits=10, random_state=None)\n",
    "    #clf1 = svm.SVC(gamma=0.0001, C=100.)\n",
    "    #scores = cross_val_score(clf1, final_train_drop_F20_std, final_target_dropF20, cv=k_fold, scoring='accuracy')\n",
    "    #print(\"Average accuracy using cross validation\",np.mean(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDecisionTree(final_train_drop_F20_std,final_target_dropF20):\n",
    "    from sklearn.model_selection import KFold, cross_val_score\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=50)\n",
    "   # clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(final_train_drop_F20_std, final_target_dropF20, test_size=0.2,random_state=0) \n",
    "    print(\"DecisionTree Analysis\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Data is divided in to training and testing in 80:20 ratio\")\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "    clf1 =  tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=50)\n",
    "    clf1.fit(final_train_drop_F20_std, final_target_dropF20)\n",
    "    Y_predict1=clf1.predict(final_train_drop_F20_std)\n",
    "\n",
    "    print(\"Accuracy of data tested with 80:20 ratio:\",accuracy_score(y_test, y_pred))\n",
    "    k_fold = KFold(n_splits=10, random_state=None)\n",
    "    #k_fold = KFold(n_splits=10)\n",
    "    score_tree = cross_val_score(clf, final_train_drop_F20_std, final_target_dropF20, cv=k_fold)\n",
    "    print('Average accuracy using cross validation', np.mean(score_tree))\n",
    "    print(classification_report(final_target_dropF20, Y_predict1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyNaiveBayes(final_train_drop_F20_std,final_target_dropF20):\n",
    "\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Naive Bayes\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    classifier = GaussianNB()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(final_train_nonstd1, final_target_dropF20, test_size=0.2,random_state=0) \n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"NaiveBayes Analysis\")\n",
    "    print(\"Data is divided in to training and testing in 80:20 ratio\")\n",
    "    # Summary of the predictions made by the classifier\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "    clf1 =GaussianNB()\n",
    "    clf1.fit(final_train_drop_F20_std, final_target_dropF20)\n",
    "    Y_predict1=clf1.predict(final_train_drop_F20_std)\n",
    "\n",
    "    # Accuracy score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print(\"Accuracy of data tested with 80:20 ratio: \",accuracy_score(y_test, y_pred))\n",
    "    #k_fold = KFold(n_splits=10)\n",
    "    k_fold = KFold(n_splits=10, random_state=None)\n",
    "\n",
    "    score_tree = cross_val_score(classifier, final_train_nonstd1, final_target_dropF20, cv=k_fold)\n",
    "    print('Average accuracy using cross validation', np.mean(score_tree))\n",
    "    print(classification_report(final_target_dropF20, Y_predict1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def applyLogisticRegression(final_train_drop_F20_std,final_target_dropF20):\n",
    " #   from sklearn.linear_model import LogisticRegression\n",
    "  #  from sklearn.model_selection import RepeatedKFold\n",
    "#\n",
    " #   X_train, X_test, y_train, y_test = train_test_split(final_train_nonstd1, final_target_dropF20, test_size=0.2,random_state=0) \n",
    "    \n",
    "  #  clf = LogisticRegression(random_state=42).fit(X_train,y_train)\n",
    "   # y_pred = clf.predict(X_test)\n",
    "    #from sklearn.metrics import accuracy_score\n",
    "   # print(\"Logistic Regression Analysis\")\n",
    "   # print(\"Data is divided in to training and testing in 80:20 ratio\")\n",
    "    #from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "\n",
    "    #print(\"Accuracy of data tested with 80:20 ratio:\",accuracy_score(y_test, y_pred))\n",
    "\n",
    "    #k_fold = KFold(n_splits=10)\n",
    "   # k_fold = KFold(n_splits=10, random_state=None)\n",
    "\n",
    "    #score_tree = cross_val_score(clf, final_train_nonstd1, final_target_dropF20, cv=k_fold)\n",
    "    #print('Average accuracy using k fold cross validation', np.mean(score_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_final_train_nonstd = train1.drop('Class', axis=1)\n",
    "impute_final_train_std = train.drop('Class', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analysis after dropping F20 \n",
      "coming in again\n",
      "[0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0\n",
      " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.66        53\n",
      "           1       0.61      0.53      0.57        47\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.62      0.62      0.61       100\n",
      "weighted avg       0.62      0.62      0.62       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.62\n",
      "Data is divided using cross validation for training and testing\n",
      "0.611889555822329\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       279\n",
      "           1       0.79      0.65      0.71       221\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.78      0.76      0.76       500\n",
      "weighted avg       0.77      0.77      0.77       500\n",
      "\n",
      "NaiveBayes Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.70      0.61        53\n",
      "           1       0.48      0.32      0.38        47\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.51      0.51      0.50       100\n",
      "weighted avg       0.51      0.52      0.50       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio:  0.52\n",
      "Average accuracy using cross validation 0.516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69       279\n",
      "           1       0.59      0.42      0.49       221\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.61      0.59      0.59       500\n",
      "weighted avg       0.61      0.61      0.60       500\n",
      "\n",
      "DecisionTree Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.81      0.70        53\n",
      "           1       0.67      0.43      0.52        47\n",
      "\n",
      "    accuracy                           0.63       100\n",
      "   macro avg       0.64      0.62      0.61       100\n",
      "weighted avg       0.64      0.63      0.61       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.63\n",
      "Average accuracy using cross validation 0.5720000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70       279\n",
      "           1       0.62      0.46      0.53       221\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.63      0.62      0.62       500\n",
      "weighted avg       0.63      0.64      0.63       500\n",
      "\n",
      "non standard\n",
      "coming in again\n",
      "[1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0\n",
      " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
      " 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1\n",
      " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
      " 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1\n",
      " 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66        53\n",
      "           1       0.62      0.62      0.62        47\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.64      0.64      0.64       100\n",
      "weighted avg       0.64      0.64      0.64       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.64\n",
      "Data is divided using cross validation for training and testing\n",
      "0.5898879551820728\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78       279\n",
      "           1       0.73      0.71      0.72       221\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.75      0.75      0.75       500\n",
      "weighted avg       0.76      0.76      0.76       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# create the Labelencoder object\n",
    "k = preprocessing.LabelEncoder()\n",
    "\n",
    "#convert the categorical columns into numeric\n",
    "final_train_drop_F20_target = k.fit_transform(train[\"Class\"])\n",
    "final_target_dropF20= pd.DataFrame(final_train_drop_F20_target,columns=[\"Class\"])\n",
    "final_target_dropF20\n",
    "print(\" Analysis after dropping F20 \")\n",
    "applyKNN(final_train_std1,final_target_dropF20)\n",
    "#applySVM(final_train_std1,final_target_dropF20)\n",
    "applyNaiveBayes(final_train_nonstd1,final_target_dropF20)\n",
    "#applyLogisticRegression(final_train_nonstd1,final_target_dropF20)\n",
    "applyDecisionTree(final_train_nonstd1,final_target_dropF20)\n",
    "print(\"non standard\")\n",
    "applyKNN(final_train_nonstd1,final_target_dropF20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coming in 2\n",
      "Analysis imputing F20 with mean \n",
      "coming in again\n",
      "[0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
      " 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1\n",
      " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67        53\n",
      "           1       0.62      0.53      0.57        47\n",
      "\n",
      "    accuracy                           0.63       100\n",
      "   macro avg       0.63      0.62      0.62       100\n",
      "weighted avg       0.63      0.63      0.63       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.63\n",
      "Data is divided using cross validation for training and testing\n",
      "0.6177278911564625\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       279\n",
      "           1       0.81      0.67      0.73       221\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.79      0.77      0.78       500\n",
      "weighted avg       0.79      0.78      0.78       500\n",
      "\n",
      "NaiveBayes Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.70      0.61        53\n",
      "           1       0.48      0.32      0.38        47\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.51      0.51      0.50       100\n",
      "weighted avg       0.51      0.52      0.50       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio:  0.52\n",
      "Average accuracy using cross validation 0.516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69       279\n",
      "           1       0.59      0.38      0.46       221\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.60      0.58      0.58       500\n",
      "weighted avg       0.60      0.61      0.59       500\n",
      "\n",
      "DecisionTree Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62        53\n",
      "           1       0.52      0.34      0.41        47\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.53      0.53      0.52       100\n",
      "weighted avg       0.53      0.54      0.52       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.54\n",
      "Average accuracy using cross validation 0.5600000000000002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       279\n",
      "           1       0.59      0.58      0.59       221\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.63      0.63      0.63       500\n",
      "weighted avg       0.64      0.64      0.64       500\n",
      "\n",
      "non standard\n",
      "coming in again\n",
      "[1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0\n",
      " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
      " 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1\n",
      " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1\n",
      " 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66        53\n",
      "           1       0.62      0.62      0.62        47\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.64      0.64      0.64       100\n",
      "weighted avg       0.64      0.64      0.64       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.64\n",
      "Data is divided using cross validation for training and testing\n",
      "0.5859271708683473\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       279\n",
      "           1       0.73      0.71      0.72       221\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.75      0.75      0.75       500\n",
      "weighted avg       0.75      0.75      0.75       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean=impute_final_train_nonstd.F20.mean()\n",
    "impute_final_train_nonstd.F20=impute_final_train_nonstd.F20.fillna(mean)\n",
    "\n",
    "mean1=impute_final_train_std.F20.mean()\n",
    "impute_final_train_std.F20=impute_final_train_std.F20.fillna(mean1)\n",
    "print(\"Coming in 2\")\n",
    "print(\"Analysis imputing F20 with mean \")\n",
    "applyKNN(impute_final_train_std,final_target_dropF20)\n",
    "#applySVM(impute_final_train_std,final_target_dropF20)\n",
    "applyNaiveBayes(impute_final_train_nonstd,final_target_dropF20)\n",
    "#applyLogisticRegression(impute_final_train_nonstd,final_target_dropF20)\n",
    "applyDecisionTree(impute_final_train_nonstd,final_target_dropF20)\n",
    "print(\"non standard\")\n",
    "applyKNN(impute_final_train_nonstd,final_target_dropF20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis imputing F20 with median \n",
      "coming in again\n",
      "[0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
      " 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1\n",
      " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67        53\n",
      "           1       0.62      0.53      0.57        47\n",
      "\n",
      "    accuracy                           0.63       100\n",
      "   macro avg       0.63      0.62      0.62       100\n",
      "weighted avg       0.63      0.63      0.63       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.63\n",
      "Data is divided using cross validation for training and testing\n",
      "0.6177278911564625\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       279\n",
      "           1       0.81      0.67      0.73       221\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.79      0.77      0.78       500\n",
      "weighted avg       0.79      0.78      0.78       500\n",
      "\n",
      "NaiveBayes Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.70      0.61        53\n",
      "           1       0.48      0.32      0.38        47\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.51      0.51      0.50       100\n",
      "weighted avg       0.51      0.52      0.50       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio:  0.52\n",
      "Average accuracy using cross validation 0.516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69       279\n",
      "           1       0.59      0.38      0.46       221\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.60      0.58      0.58       500\n",
      "weighted avg       0.60      0.61      0.59       500\n",
      "\n",
      "DecisionTree Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62        53\n",
      "           1       0.52      0.34      0.41        47\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.53      0.53      0.52       100\n",
      "weighted avg       0.53      0.54      0.52       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.54\n",
      "Average accuracy using cross validation 0.5600000000000002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       279\n",
      "           1       0.59      0.58      0.59       221\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.63      0.63      0.63       500\n",
      "weighted avg       0.64      0.64      0.64       500\n",
      "\n",
      "coming in again\n",
      "[1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0\n",
      " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
      " 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1\n",
      " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1\n",
      " 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66        53\n",
      "           1       0.62      0.62      0.62        47\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.64      0.64      0.64       100\n",
      "weighted avg       0.64      0.64      0.64       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.64\n",
      "Data is divided using cross validation for training and testing\n",
      "0.5859271708683473\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       279\n",
      "           1       0.73      0.71      0.72       221\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.75      0.75      0.75       500\n",
      "weighted avg       0.75      0.75      0.75       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "median=impute_final_train_nonstd.F20.median()\n",
    "impute_final_train_nonstd.F20=impute_final_train_nonstd.F20.fillna(median)\n",
    "impute_final_train_nonstd\n",
    "median=impute_final_train_std.F20.median()\n",
    "impute_final_train_std.F20=impute_final_train_std.F20.fillna(median)\n",
    "print(\"Analysis imputing F20 with median \")\n",
    "applyKNN(impute_final_train_std,final_target_dropF20)\n",
    "#applySVM(impute_final_train_std,final_target_dropF20)\n",
    "applyNaiveBayes(impute_final_train_nonstd,final_target_dropF20)\n",
    "#applyLogisticRegression(impute_final_train_nonstd,final_target_dropF20)\n",
    "applyDecisionTree(impute_final_train_nonstd,final_target_dropF20)\n",
    "applyKNN(impute_final_train_nonstd,final_target_dropF20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(data_1):\n",
    "    data_2,outliers1=detect_outlier(data_1)\n",
    "    while(len(outliers1)!=0):\n",
    "       data_2,outliers1=detect_outlier(data_2)\n",
    "       return data_2\n",
    "def detect_outlier(data_3):\n",
    "    outliers=[]\n",
    "    outlier_index=[]\n",
    "    threshold=3\n",
    "    mean_1 = np.mean(data_3)\n",
    "    std_1 =np.std(data_3)\n",
    "    med=data_3.median()\n",
    "\n",
    "    for ind,y in enumerate(data_3):\n",
    "        z_score= (y - mean_1)/std_1 \n",
    "        if np.abs(z_score) >threshold:\n",
    "            outliers.append(y)\n",
    "            outlier_index.append(ind)\n",
    "            data_3[ind]=med\n",
    "    return data_3,outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_std(data_1):\n",
    "    data_2,outliers1=detect_outlier_std(data_1)\n",
    "    while(len(outliers1)!=0):\n",
    "       data_2,outliers1=detect_outlier_std(data_2)\n",
    "       return data_2\n",
    "def detect_outlier_std(data_3):\n",
    "    outliers=[]\n",
    "    outlier_index=[]\n",
    "    threshold=3\n",
    "    mean_1 = np.mean(data_3)\n",
    "    std_1 =np.std(data_3)\n",
    "    med=data_3.median()\n",
    "\n",
    "    for ind,y in enumerate(data_3):\n",
    "        if np.abs(y) >threshold:\n",
    "            outliers.append(y)\n",
    "            outlier_index.append(ind)\n",
    "            data_3[ind]=med\n",
    "    return data_3,outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming in again\n",
      "[0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
      " 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0\n",
      " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1]\n",
      "KNN analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.70      0.67        53\n",
      "           1       0.63      0.57      0.60        47\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.64      0.64      0.64       100\n",
      "weighted avg       0.64      0.64      0.64       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.64\n",
      "Data is divided using cross validation for training and testing\n",
      "0.5998847539015606\n",
      "classification report below\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81       279\n",
      "           1       0.79      0.67      0.72       221\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.77      0.76      0.76       500\n",
      "weighted avg       0.77      0.77      0.77       500\n",
      "\n",
      "NaiveBayes Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.64      0.57        53\n",
      "           1       0.42      0.30      0.35        47\n",
      "\n",
      "    accuracy                           0.48       100\n",
      "   macro avg       0.47      0.47      0.46       100\n",
      "weighted avg       0.47      0.48      0.46       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio:  0.48\n",
      "Average accuracy using cross validation 0.484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69       279\n",
      "           1       0.58      0.44      0.50       221\n",
      "\n",
      "    accuracy                           0.61       500\n",
      "   macro avg       0.61      0.60      0.59       500\n",
      "weighted avg       0.61      0.61      0.60       500\n",
      "\n",
      "Logistic Regression Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.66      0.62        53\n",
      "           1       0.56      0.49      0.52        47\n",
      "\n",
      "    accuracy                           0.58       100\n",
      "   macro avg       0.58      0.57      0.57       100\n",
      "weighted avg       0.58      0.58      0.58       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.58\n",
      "Average accuracy using k fold cross validation 0.52\n",
      "DecisionTree Analysis\n",
      "Data is divided in to training and testing in 80:20 ratio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.81      0.70        53\n",
      "           1       0.68      0.45      0.54        47\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.65      0.63      0.62       100\n",
      "weighted avg       0.65      0.64      0.63       100\n",
      "\n",
      "Accuracy of data tested with 80:20 ratio: 0.64\n",
      "Average accuracy using cross validation 0.558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.87      0.73       279\n",
      "           1       0.68      0.35      0.46       221\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.65      0.61      0.60       500\n",
      "weighted avg       0.65      0.64      0.61       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier(final_train_nonstd1.F1)\n",
    "outlier(final_train_nonstd1.F2)\n",
    "outlier(final_train_nonstd1.F3)\n",
    "outlier(final_train_nonstd1.F4)\n",
    "outlier(final_train_nonstd1.F5)\n",
    "outlier(final_train_nonstd1.F6)\n",
    "outlier(final_train_nonstd1.F7)\n",
    "outlier(final_train_nonstd1.F8)\n",
    "outlier(final_train_nonstd1.F9)\n",
    "outlier(final_train_nonstd1.F10)\n",
    "outlier(final_train_nonstd1.F11)\n",
    "outlier(final_train_nonstd1.F12)\n",
    "outlier(final_train_nonstd1.F13)\n",
    "outlier(final_train_nonstd1.F14)\n",
    "outlier(final_train_nonstd1.F15)\n",
    "outlier(final_train_nonstd1.F16)\n",
    "outlier(final_train_nonstd1.F17)\n",
    "outlier(final_train_nonstd1.F18)\n",
    "outlier(final_train_nonstd1.F19)\n",
    "\n",
    "outlier_std(final_train_std1.F1)\n",
    "outlier_std(final_train_std1.F2)\n",
    "outlier_std(final_train_std1.F3)\n",
    "outlier_std(final_train_std1.F4)\n",
    "outlier_std(final_train_std1.F5)\n",
    "outlier_std(final_train_std1.F6)\n",
    "outlier_std(final_train_std1.F7)\n",
    "outlier_std(final_train_std1.F8)\n",
    "outlier_std(final_train_std1.F9)\n",
    "outlier_std(final_train_std1.F10)\n",
    "outlier_std(final_train_std1.F11)\n",
    "outlier_std(final_train_std1.F12)\n",
    "outlier_std(final_train_std1.F13)\n",
    "outlier_std(final_train_std1.F14)\n",
    "outlier_std(final_train_std1.F15)\n",
    "outlier_std(final_train_std1.F16)\n",
    "outlier_std(final_train_std1.F17)\n",
    "outlier_std(final_train_std1.F18)\n",
    "outlier_std(final_train_std1.F19)\n",
    "\n",
    "applyKNN(final_train_std1,final_target_dropF20)\n",
    "#applySVM(final_train_std1,final_target_dropF20)\n",
    "applyNaiveBayes(final_train_nonstd1,final_target_dropF20)\n",
    "applyLogisticRegression(final_train_nonstd1,final_target_dropF20)\n",
    "applyDecisionTree(final_train_nonstd1,final_target_dropF20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
      "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20'],\n",
      "      dtype='object')\n",
      "Index(['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
      "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'Class'],\n",
      "      dtype='object')\n",
      "           F1        F2        F3        F4        F5        F6        F7  \\\n",
      "0   -1.418467 -1.012073 -1.098985  1.361172  0.536097 -2.297206 -2.332587   \n",
      "1   -1.418467 -1.012073 -0.102548 -0.544226  2.679331  0.096177 -0.283861   \n",
      "2    0.704987  0.988071  1.022003  0.519404  1.045475  0.057730  0.984744   \n",
      "3    0.704987  0.988071  1.392108 -0.935746  0.795592  0.278805  0.015063   \n",
      "4    0.704987  0.988071 -1.141689  0.206188 -1.107370 -0.903469  1.057653   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "495  0.704987  0.988071  0.637663  0.930500  0.420766  0.970868  2.027334   \n",
      "496 -1.418467 -1.012073  0.125209  1.387273  0.122828  0.769016  1.757573   \n",
      "497  0.704987 -1.012073  1.676805  0.160510  1.679796  0.249969  0.000481   \n",
      "498  0.704987 -1.012073  0.011331 -0.518125  1.055086  0.596000  0.022354   \n",
      "499  0.704987  0.988071  0.153679  0.193137  1.689407 -1.393680  1.633629   \n",
      "\n",
      "           F8        F9       F10  ...       F12       F13       F14  \\\n",
      "0   -0.666611  1.843313 -0.069223  ... -0.064644  0.204608  0.923256   \n",
      "1   -0.013371  2.712652 -0.368632  ...  0.172147  0.176107 -0.566076   \n",
      "2   -1.636263  0.104637 -0.129105  ...  0.043885 -0.735921 -0.436568   \n",
      "3    1.425798 -0.843732 -0.308750  ...  1.721155  1.145137  0.923256   \n",
      "4    0.272421 -1.041309  0.050540  ... -0.380365  0.446865  1.570792   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "495  0.935868 -0.092940  0.529594  ... -0.913145 -0.336909  1.441285   \n",
      "496  1.293108  0.736883 -0.548278  ... -0.370499 -0.536415 -0.436568   \n",
      "497  1.231867  0.341729 -3.362721  ... -0.755285 -0.222905  0.405228   \n",
      "498  1.323729 -0.606640 -0.129105  ...  0.675328 -1.177685 -0.566076   \n",
      "499  0.599041 -0.290517 -1.566268  ... -0.755285  1.287642 -0.889843   \n",
      "\n",
      "          F15       F16       F17       F18       F19           F20  Class  \n",
      "0   -0.646145 -2.357588 -1.641155  0.103485  1.034164 -3.723309e-17   True  \n",
      "1   -1.236790 -0.027883  0.442592  0.694428  0.138754 -3.723309e-17   True  \n",
      "2   -0.473025 -0.611830  1.780685  2.687608  0.293729  7.814907e-01  False  \n",
      "3    0.474044  0.148517  0.156587  0.273757 -0.515584  4.637801e-01  False  \n",
      "4   -0.096234 -1.700648 -0.466494 -0.687777  0.207632 -3.723309e-17   True  \n",
      "..        ...       ...       ...       ...       ...           ...    ...  \n",
      "495  0.229639 -0.988963 -0.589067  1.345467  1.998451  1.480454e+00   True  \n",
      "496 -0.086051 -1.019377  0.044229  0.554204  1.774599  9.297557e-01  False  \n",
      "497 -0.279538 -0.508423 -0.834214  0.083453 -0.584461 -5.846651e-01  False  \n",
      "498  0.423126  0.726381 -0.732069 -1.118464 -0.498364 -3.723309e-17   True  \n",
      "499  1.390562 -1.183612  0.187231  0.073437  1.550746  1.893478e+00   True  \n",
      "\n",
      "[500 rows x 21 columns]\n",
      "           F1        F2        F3        F4        F5        F6        F7  \\\n",
      "0   -1.418467 -1.012073 -1.098985  1.361172  0.536097 -2.297206 -2.332587   \n",
      "1   -1.418467 -1.012073 -0.102548 -0.544226  2.679331  0.096177 -0.283861   \n",
      "2    0.704987  0.988071  1.022003  0.519404  1.045475  0.057730  0.984744   \n",
      "3    0.704987  0.988071  1.392108 -0.935746  0.795592  0.278805  0.015063   \n",
      "4    0.704987  0.988071 -1.141689  0.206188 -1.107370 -0.903469  1.057653   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "495  0.704987  0.988071  0.637663  0.930500  0.420766  0.970868  2.027334   \n",
      "496 -1.418467 -1.012073  0.125209  1.387273  0.122828  0.769016  1.757573   \n",
      "497  0.704987 -1.012073  1.676805  0.160510  1.679796  0.249969  0.000481   \n",
      "498  0.704987 -1.012073  0.011331 -0.518125  1.055086  0.596000  0.022354   \n",
      "499  0.704987  0.988071  0.153679  0.193137  1.689407 -1.393680  1.633629   \n",
      "\n",
      "           F8        F9       F10  ...       F12       F13       F14  \\\n",
      "0   -0.666611  1.843313 -0.069223  ... -0.064644  0.204608  0.923256   \n",
      "1   -0.013371  2.712652 -0.368632  ...  0.172147  0.176107 -0.566076   \n",
      "2   -1.636263  0.104637 -0.129105  ...  0.043885 -0.735921 -0.436568   \n",
      "3    1.425798 -0.843732 -0.308750  ...  1.721155  1.145137  0.923256   \n",
      "4    0.272421 -1.041309  0.050540  ... -0.380365  0.446865  1.570792   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "495  0.935868 -0.092940  0.529594  ... -0.913145 -0.336909  1.441285   \n",
      "496  1.293108  0.736883 -0.548278  ... -0.370499 -0.536415 -0.436568   \n",
      "497  1.231867  0.341729 -3.362721  ... -0.755285 -0.222905  0.405228   \n",
      "498  1.323729 -0.606640 -0.129105  ...  0.675328 -1.177685 -0.566076   \n",
      "499  0.599041 -0.290517 -1.566268  ... -0.755285  1.287642 -0.889843   \n",
      "\n",
      "          F15       F16       F17       F18       F19           F20  Class  \n",
      "0   -0.646145 -2.357588 -1.641155  0.103485  1.034164 -3.723309e-17   True  \n",
      "1   -1.236790 -0.027883  0.442592  0.694428  0.138754 -3.723309e-17   True  \n",
      "2   -0.473025 -0.611830  1.780685  2.687608  0.293729  7.814907e-01  False  \n",
      "3    0.474044  0.148517  0.156587  0.273757 -0.515584  4.637801e-01  False  \n",
      "4   -0.096234 -1.700648 -0.466494 -0.687777  0.207632 -3.723309e-17   True  \n",
      "..        ...       ...       ...       ...       ...           ...    ...  \n",
      "495  0.229639 -0.988963 -0.589067  1.345467  1.998451  1.480454e+00   True  \n",
      "496 -0.086051 -1.019377  0.044229  0.554204  1.774599  9.297557e-01  False  \n",
      "497 -0.279538 -0.508423 -0.834214  0.083453 -0.584461 -5.846651e-01  False  \n",
      "498  0.423126  0.726381 -0.732069 -1.118464 -0.498364 -3.723309e-17   True  \n",
      "499  1.390562 -1.183612  0.187231  0.073437  1.550746  1.893478e+00   True  \n",
      "\n",
      "[500 rows x 21 columns]\n",
      "10-fold cross validation average accuracy: 0.608\n",
      "     F1  F2   F3    F4    F5    F6    F7    F8   F9  F10  ...   F12   F13  \\\n",
      "0     0   0  155  1.23 -0.75 -0.22  2.07 -0.19   18    6  ...  0.82  0.07   \n",
      "1     0   0  127 -0.30  0.75 -1.34  0.17  0.39   67    8  ... -0.77  1.15   \n",
      "2     1   1   22  1.06 -1.05  0.83  0.62 -0.63   15   23  ... -0.44  1.31   \n",
      "3     1   1   29 -1.01  1.06  0.96  2.19 -2.07    8  -26  ... -0.76  1.16   \n",
      "4     0   0   78  0.10 -1.07  2.20  0.99  1.84   83   29  ...  1.00  0.86   \n",
      "..   ..  ..  ...   ...   ...   ...   ...   ...  ...  ...  ...   ...   ...   \n",
      "495   1   0  131 -1.30 -0.62  1.27  1.21  0.37   38   15  ... -1.66  1.03   \n",
      "496   0   1  236  0.22  1.45  1.10  1.24 -0.53  106   19  ... -1.16  1.38   \n",
      "497   0   0   65 -0.67 -0.52  1.26  1.40  1.87   40   -9  ...  0.36  0.50   \n",
      "498   1   1  106  0.17 -0.74 -0.40  2.21  1.87   47    9  ... -0.23  0.79   \n",
      "499   1   1  106  0.25 -0.14  0.81 -0.18  1.85   38  -14  ... -0.93  1.02   \n",
      "\n",
      "     F14   F15   F16   F17   F18   F19   F20  Class  \n",
      "0    -26 -0.03  0.59 -0.32 -1.55   6.4  1.51  False  \n",
      "1     19  1.93  1.53 -0.13  0.79   3.6 -0.73   True  \n",
      "2     -5  0.38  0.24 -1.46  0.52   3.7  1.49  False  \n",
      "3     -1 -0.10  0.15  1.29  0.79  13.1  2.25  False  \n",
      "4     -5 -1.02 -1.02 -0.43  0.23  10.4 -2.91  False  \n",
      "..   ...   ...   ...   ...   ...   ...   ...    ...  \n",
      "495   -7 -0.98  3.55  1.62  0.53   5.3   NaN  False  \n",
      "496   -5 -1.74  0.05 -0.13  0.79   5.2   NaN   True  \n",
      "497    3  0.09 -1.06 -0.75  1.78   7.8   NaN   True  \n",
      "498    4 -0.46 -1.47 -0.83  0.79   9.1  1.07  False  \n",
      "499   -5 -1.16  1.34  0.24 -0.22   4.2  0.57   True  \n",
      "\n",
      "[500 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "final_test_std_prediction=test.drop('Class', axis=1)\n",
    "mean2=final_test_std_prediction.F20.mean()\n",
    "final_test_std_prediction.F20=final_test_std_prediction.F20.fillna(mean2)\n",
    "\n",
    "impute_final_train_std_prediction = train\n",
    "mean1=impute_final_train_std_prediction.F20.mean()\n",
    "impute_final_train_std_prediction.F20=impute_final_train_std_prediction.F20.fillna(mean1)\n",
    "\n",
    "print(final_test_std_prediction.columns)\n",
    "print(impute_final_train_std_prediction.columns)\n",
    "\n",
    "target=\"Class\"\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "predictors = [x for x in final_test_std_prediction.columns if x not in [target]]\n",
    "# print predictors\n",
    "print(train)\n",
    "\n",
    "print(impute_final_train_std_prediction)\n",
    "\n",
    "alg1 = KNeighborsClassifier(n_neighbors=5)\n",
    "calPredictAccuracy(alg1, impute_final_train_std_prediction, final_test_std_prediction, predictors, target)\n",
    "target=test[\"Class\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def calPredictAccuracy(alg, dtrain1, dtest1, predictors1, target):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    \n",
    "    alg.fit(dtrain1[predictors1], dtrain1[target])\n",
    "    \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain1[predictors1])\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "    \n",
    "    #CrossValidation:\n",
    "    cv_train_score = model_selection.cross_val_score(alg, dtrain1[predictors1], dtrain1[target], cv=kfold, scoring='accuracy')\n",
    "    print(\"10-fold cross validation average accuracy: %.3f\" % (cv_train_score.mean()))\n",
    "     \n",
    "        \n",
    "    #Predict on testing data:\n",
    "    dtest1[target] = alg.predict(dtest1[predictors1])\n",
    "    test_nonstd = pd.read_csv(\"CE802_Ass_2019_Test.csv\")\n",
    "    test_nonstd=test_nonstd.drop('Class', axis=1)\n",
    "    test_nonstd[target]=alg.predict(dtest1[predictors1])\n",
    "    \n",
    "   # print(classification_report(dtrain1[target].values, dtest1[target]))\n",
    "    print(test_nonstd)\n",
    "    test_nonstd.to_csv('CE802_Ass_2019_Test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'impute_final_train_nonstd2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-338-ddae8c42a5c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimpute_final_train_nonstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimpute_final_train_nonstd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF20\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimpute_final_train_nonstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimpute_final_train_nonstd2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmean1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimpute_final_train_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'impute_final_train_nonstd2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
